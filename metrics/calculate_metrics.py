import os
import json
import logging
import time
from dotenv import load_dotenv
from mistralai import Mistral
import sys

load_dotenv()
logger = logging.getLogger("CalculateMetrics")
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, ROOT_DIR)

SRC_DIR = os.path.join(ROOT_DIR, 'src')
sys.path.insert(0, SRC_DIR)

from src.main import setup_terraria_rag
from src.agent import MistralLLM


#############################################
# 0 ‚Äî Retry Helpers
#############################################

def safe_eval_call(client, q, gt, my_answer, baseline_answer):
    """
    –î–µ–ª–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç.
    """
    while True:
        try:
            return evaluate_answer(client, q, gt, my_answer, baseline_answer)
        except Exception as e:
            logger.warning(f"Baseline error: {e}, retrying in 30s...")
            time.sleep(30)


def safe_run_rag(rag, question):
    """
    –î–µ–ª–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç.
    """
    while True:
        try:
            return rag.run(question)
        except Exception as e:
            logger.warning(f"RAG error: {e}, retrying in 30s...")
            time.sleep(30)


def safe_baseline_answer(baseline, question):
    """
    –î–µ–ª–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç.
    """
    while True:
        try:
            return baseline.call(system_prompt="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–≥—Ä–µ Terraria. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: ", user_prompt=question)
        except Exception as e:
            logger.warning(f"Baseline error: {e}, retrying in 30s...")
            time.sleep(30)


#############################################
# 1 ‚Äî –û—Ü–µ–Ω—â–∏–∫ (–±–æ–ª—å—à–∞—è LLM-–º–æ–¥–µ–ª—å)
#############################################
def evaluate_answer(eval_client, question, groundtruth, my_answer, baseline_answer):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –∏ baseline —Å –ø–æ–º–æ—â—å—é LLM, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ .call().
    """
    system_prompt = """
–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –æ—Ü–µ–Ω—â–∏–∫ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ Terraria.
–°—Ä–∞–≤–Ω–∏ –¥–≤–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è groundtruth.
–°–∏–ª—å–Ω–æ –Ω–∞–∫–∞–∑—ã–≤–∞–π –ª–æ–∂—å —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–∏.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ç–æ –Ω–µ –Ω–∞–∫–∞–∑—ã–≤–∞–π –º–æ–¥–µ–ª—å.
–í–æ—Ç –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏.
0 - –æ—Ç–≤–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–µ—Ä–µ–Ω.
1 - –æ—Ç–≤–µ—Ç –≤–µ—Ä–µ–Ω –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–æ
2 - –æ—Ç–≤–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –≤–µ—Ä–µ–Ω, –Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏.
3 - –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–µ—Ä–µ–Ω, –Ω–æ –Ω–µ –ø–æ–ª–æ–Ω.
4 - –æ—Ç–≤–µ—Ç –≤–µ—Ä–µ–Ω, –Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–ª–æ–Ω.
5 - –æ—Ç–≤–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–µ—Ä–µ–Ω –∏ –ø–æ–ª–æ–Ω.
–í–µ—Ä–Ω–∏ JSON —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:

{
    "rag_score": int,      # 0‚Äì5
    "baseline_score": int  # 0‚Äì5
}
"""

    user_prompt = f"""
Question: {question}

Groundtruth:
{groundtruth}

--- My Model Answer ---
{my_answer}

--- Baseline Answer ---
{baseline_answer}
"""

    # –í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ .call()
    response = eval_client.call(system_prompt=system_prompt, user_prompt=user_prompt)

    text = response  # call() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é

    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1:
        raise ValueError("Evaluator returned non-JSON result")

    json_text = text[start:end+1]
    return json.loads(json_text)


#############################################
# 2 ‚Äî –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
#############################################

def calculate_metrics():
    logging.basicConfig(level=logging.INFO)
    logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫...")
    with open("metrics/benchmark_questions.json", "r", encoding="utf-8") as f:
        questions = json.load(f)

    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TerrariaRAG...")
    terraria_rag = setup_terraria_rag()

    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è baseline –º–æ–¥–µ–ª–∏...")
    baseline = MistralLLM(
        mistral_client=Mistral(
            api_key=os.getenv("API_KEY"),
        ),
        model_name="ministral-8b-2410",
    )

    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏-–æ—Ü–µ–Ω—â–∏–∫–∞...")
    eval_client = MistralLLM(
        mistral_client=Mistral(
            api_key=os.getenv("API_KEY"),
        ),
        model_name="ministral-8b-2410",
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    results = load_existing_results()

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    processed_questions = {r["question"] for r in results}

    for i, item in enumerate(questions):
        q = item["question"]
        theme = item.get("theme", "")
        gt = item["groundtruth"]

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if q in processed_questions:
            logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {q}")
            continue

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ ({i+1}/{len(questions)}): {q}")

        try:
            my_answer = safe_run_rag(terraria_rag, q)
        except Exception as e:
            logger.error(f"My model error: {e}")
            my_answer = ""

        try:
            baseline_answer = safe_baseline_answer(baseline, q)
        except Exception as e:
            logger.error(f"Baseline error: {e}")
            baseline_answer = ""

        try:
            eval_result = safe_eval_call(
                eval_client, q, gt, my_answer, baseline_answer
            )
        except Exception as e:
            logger.error(f"Evaluation error: {e}")
            eval_result = {
                "rag_score": 0,
                "baseline_score": 0
            }

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        new_result = {
            "question": q,
            "theme": theme,
            "groundtruth": gt,
            "my_model_answer": my_answer,
            "baseline_answer": baseline_answer,
            "evaluation": eval_result
        }
        results.append(new_result)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        save_results(results)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {i+1}/{len(questions)}")

    logger.info("üéâ –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã! –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ metrics/out/model_evaluation.json")

def load_existing_results():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å"""
    os.makedirs("metrics/out", exist_ok=True)
    results_file = "metrics/out/model_evaluation.json"

    if os.path.exists(results_file):
        try:
            with open(results_file, "r", encoding="utf-8") as f:
                results = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(results)} –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")

    return []

def save_results(results):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª"""
    os.makedirs("metrics/out", exist_ok=True)
    results_file = "metrics/out/model_evaluation.json"

    try:
        temp_file = results_file + ".tmp"
        with open(temp_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        os.replace(temp_file, results_file)
        logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã ({len(results)} –∑–∞–ø–∏—Å–µ–π)")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")

if __name__ == "__main__":
    calculate_metrics()
