"""
get_data.py ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å Terraria Wiki
–ê–≤—Ç–æ—Ä: nvclon

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python data/get_data.py --mode list     # –≤—ã–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
    python data/get_data.py --mode dump     # –≤—ã–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü
    
LICENSE: blablabla
"""

import requests
import json
import time
import os
import argparse

BASE_URL = "https://terraria.wiki.gg/ru/api.php"
HEADERS = {"User-Agent": "TerrariaRAGBot/0.1 (by nvclon)"}

def get_all_pages():
    all_pages = []
    apcontinue = None
    tries = 0

    while True:
        params = {
            "action": "query",
            "list": "allpages",
            "apnamespace": 0,
            "apfilterredir": "nonredirects",
            "aplimit": "500",
            "format": "json",
        }
        if apcontinue:
            params["apcontinue"] = apcontinue

        try:
            r = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=15)
            if r.status_code == 429:
                print("‚ö†Ô∏è HTTP 429 Too Many Requests ‚Äî –∂–¥—É 15 —Å–µ–∫...")
                time.sleep(15)
                continue
            if r.status_code != 200:
                print(f"‚ö†Ô∏è HTTP {r.status_code}, –∂–¥—É 10 —Å–µ–∫...")
                time.sleep(10)
                continue

            data = r.json()
            if "error" in data:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ API: {data['error']['info']}")
                time.sleep(10)
                continue

            if "query" not in data:
                print("‚ö†Ô∏è –ù–µ—Ç –∫–ª—é—á–∞ 'query', –ø–æ–≤—Ç–æ—Ä—è—é –∑–∞–ø—Ä–æ—Å...")
                tries += 1
                if tries > 5:
                    print("‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ ‚Äî –≤—ã—Ö–æ–∂—É.")
                    break
                time.sleep(5)
                continue

            pages = data["query"]["allpages"]
            all_pages.extend(pages)
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(all_pages)}")

            if "continue" in data:
                apcontinue = data["continue"]["apcontinue"]
                time.sleep(1)
            else:
                break

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            time.sleep(5)

    return all_pages


def dump_page_list():
    os.makedirs("data", exist_ok=True)
    pages = get_all_pages()
    path = "../data/pages_list.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(pages, f, ensure_ascii=False, indent=2)
    print(f"üíæ –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {path} ({len(pages)} —à—Ç—É–∫)")


# ---------------------------------------------
# 2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü
# ---------------------------------------------
def get_page_text(title: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–∏–∫–∏-—Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    params = {
        "action": "parse",
        "prop": "revisions",
        "rvslots": "main",
        "rvprop": "content|timestamp|user",
        "titles": title,
        "format": "json",
    }

    r = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=20)

    if r.status_code == 429:
        print("‚ö†Ô∏è 429 Too Many Requests ‚Äî –∂–¥—É 20 —Å–µ–∫...")
        time.sleep(20)
        return None

    if r.status_code != 200:
        print(f"‚ö†Ô∏è HTTP {r.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {title}")
        return None

    data = r.json()
    if "query" not in data or "pages" not in data["query"]:
        print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {title}")
        return None

    page = next(iter(data["query"]["pages"].values()))
    revisions = page.get("revisions", [])
    if not revisions:
        return None

    rev = revisions[0]
    slots = rev.get("slots", {})
    main = slots.get("main", {})

    text = main.get("*", "")
    return {
        "title": title,
        "pageid": page.get("pageid"),
        "ns": page.get("ns"),
        "timestamp": rev.get("timestamp"),
        "user": rev.get("user"),
        "content": text,
    }


def dump_all_pages():
    os.makedirs("data", exist_ok=True)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü
    if not os.path.exists("../data/pages_list.json"):
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω ../data/pages_list.json. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ --mode list")
        return

    with open("../data/pages_list.json", "r", encoding="utf-8") as f:
        pages = json.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    dump_path = "../data/wiki_dump.json"
    if os.path.exists(dump_path):
        with open(dump_path, "r", encoding="utf-8") as f:
            all_data = json.load(f)
        print(f"üîÅ –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É, —É–∂–µ —Å–∫–∞—á–∞–Ω–æ {len(all_data)} —Å—Ç—Ä–∞–Ω–∏—Ü.")
    else:
        all_data = {}

    for i, page in enumerate(pages, start=1):
        title = page["title"]
        if title in all_data:
            continue  # —É–∂–µ —Å–∫–∞—á–∞–Ω–æ

        text = get_page_text(title)
        if text is None:
            continue

        all_data[title] = text
        print(f"{i}/{len(pages)}: {title}")

        # –ê–≤—Ç–æ—Å–µ–π–≤ –∫–∞–∂–¥—ã–µ 100 —Å—Ç—Ä–∞–Ω–∏—Ü
        if i % 100 == 0:
            with open(dump_path, "w", encoding="utf-8") as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            print(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(all_data)} —Å—Ç—Ä–∞–Ω–∏—Ü")

        time.sleep(2)  # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–≤–∏—Ç—å 429

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    with open(dump_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_data)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ {dump_path}")


# ---------------------------------------------
# 3Ô∏è‚É£ CLI
# ---------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–í—ã–≥—Ä—É–∑–∫–∞ Terraria Wiki –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--mode", choices=["list", "dump"], required=True, help="–†–µ–∂–∏–º: list –∏–ª–∏ dump")
    args = parser.parse_args()

    if args.mode == "list":
        dump_page_list()
    elif args.mode == "dump":
        dump_all_pages()